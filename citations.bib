@misc{acharyya_consistent_estimation_2025,
  author = {Acharyya, Aranyak and Trosset, Michael W. and Priebe, Carey E. and Helm, Hayden S.},
  doi = {10.48550/arXiv.2409.17308},
  month = {January},
  number = {2409.17308},
  publisher = {arXiv},
  title = {Consistent Estimation of Generative Model Representations in the Data Kernel Perspective Space},
  year = {2025}
}

@inproceedings{arora_stronger_generalization_2018,
  author = {Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  issn = {2640-3498},
  pages = {254--263},
  publisher = {PMLR},
  title = {Stronger {{Generalization Bounds}} for {{Deep Nets}} via a {{Compression Approach}}},
  year = {2018}
}

@inproceedings{bartlett_spectrallynormalized_margin_2017,
  author = {Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  title = {Spectrally-Normalized Margin Bounds for Neural Networks},
  volume = {30},
  year = {2017}
}

@inproceedings{bordelon_depthwise_hyperparameter_2024,
  author = {Bordelon, Blake and Noci, Lorenzo and Li, Mufan Bill and Hanin, Boris and Pehlevan, Cengiz},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  month = {January},
  title = {Depthwise {{Hyperparameter Transfer}} in {{Residual Networks}}: {{Dynamics}} and {{Scaling Limit}}},
  year = {2024}
}

@inproceedings{chen_scaling_laws_2024,
  author = {Chen, Yangyi and Huang, Binxuan and Gao, Yifan and Wang, Zhengyang and Yang, Jingfeng and Ji, Heng},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  month = {October},
  title = {Scaling {{Laws}} for {{Predicting Downstream Performance}} in {{LLMs}}},
  year = {2024}
}

@inproceedings{cohen_certified_adversarial_2019,
  author = {Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  issn = {2640-3498},
  pages = {1310--1320},
  publisher = {PMLR},
  title = {Certified {{Adversarial Robustness}} via {{Randomized Smoothing}}},
  year = {2019}
}

@inproceedings{croce_reliable_evaluation_2020,
  author = {Croce, Francesco and Hein, Matthias},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  issn = {2640-3498},
  pages = {2206--2216},
  publisher = {PMLR},
  title = {Reliable Evaluation of Adversarial Robustness with an Ensemble of Diverse Parameter-Free Attacks},
  year = {2020}
}

@inproceedings{croce_robustbench_standardized_2021,
  author = {Croce, Francesco and Andriushchenko, Maksym and Sehwag, Vikash and Debenedetti, Edoardo and Flammarion, Nicolas and Chiang, Mung and Mittal, Prateek and Hein, Matthias},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  title = {{{RobustBench}}: A Standardized Adversarial Robustness Benchmark},
  year = {2021}
}

@misc{dey_dont_be_2025,
  author = {Dey, Nolan and Zhang, Bin Claire and Noci, Lorenzo and Li, Mufan and Bordelon, Blake and Bergsma, Shane and Pehlevan, Cengiz and Hanin, Boris and Hestness, Joel},
  doi = {10.48550/arXiv.2505.01618},
  month = {May},
  number = {2505.01618},
  publisher = {arXiv},
  title = {Don't Be Lazy: {{CompleteP}} Enables Compute-Efficient Deep Transformers},
  year = {2025}
}

@inproceedings{dziugaite_computing_nonvacuous_2017,
  author = {Dziugaite, Gintare Karolina and Roy, Daniel M},
  booktitle = {Proceedings of the {{Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  title = {Computing {{Nonvacuous Generalization Bounds}} for {{Deep}} ({{Stochastic}}) {{Neural Networks}} with {{Many More Parameters}} than {{Training Data}}},
  year = {2017}
}

@misc{foret_sharpnessaware_minimization_2021,
  author = {Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  doi = {10.48550/arXiv.2010.01412},
  month = {April},
  number = {2010.01412},
  publisher = {arXiv},
  title = {Sharpness-{{Aware Minimization}} for {{Efficiently Improving Generalization}}},
  year = {2021}
}

@misc{hall_introducing_marin_2025,
  author = {Hall, David},
  howpublished = {http://marin.community/blog/2025/05/19/announcement/},
  journal = {Marin},
  month = {May},
  title = {Introducing {{Marin}}: {{An Open Lab}} for {{Building Foundation Models}}},
  year = {2025}
}

@inproceedings{hendrycks_measuring_massive_2021,
  author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  month = {January},
  title = {Measuring {{Massive Multitask Language Understanding}}},
  year = {2021}
}

@inproceedings{hoffmann_training_computeoptimal_2022,
  address = {Red Hook, NY, USA},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and {de Las Casas}, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  isbn = {978-1-7138-7108-8},
  pages = {30016--30030},
  title = {Training Compute-Optimal Large Language Models},
  year = {2022}
}

@inproceedings{jacot_neural_tangent_2018,
  author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  title = {Neural {{Tangent Kernel}}: {{Convergence}} and {{Generalization}} in {{Neural Networks}}},
  volume = {31},
  year = {2018}
}

@inproceedings{jiang_fantastic_generalization_2019,
  author = {Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  month = {September},
  title = {Fantastic {{Generalization Measures}} and {{Where}} to {{Find Them}}},
  year = {2019}
}

@misc{kaplan_scaling_laws_2020,
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  doi = {10.48550/arXiv.2001.08361},
  month = {January},
  number = {2001.08361},
  publisher = {arXiv},
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  year = {2020}
}

@inproceedings{lee_holistic_evaluation_2023a,
  author = {Lee, Tony and Yasunaga, Michihiro and Meng, Chenlin and Mai, Yifan and Park, Joon Sung and Gupta, Agrim and Zhang, Yunzhi and Narayanan, Deepak and Teufel, Hannah and Bellagente, Marco and others},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  pages = {69981--70011},
  title = {Holistic {{Evaluation}} of {{Text-to-Image Models}}},
  volume = {36},
  year = {2023}
}

@article{liang_holistic_evaluation_2023a,
  author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  issn = {2835-8856},
  journal = {Transactions on Machine Learning Research},
  month = {February},
  title = {Holistic {{Evaluation}} of {{Language Models}}},
  year = {2023}
}

@article{martin_implicit_selfregularization_2021,
  author = {Martin, Charles H and Mahoney, Michael W},
  journal = {Journal of Machine Learning Research},
  month = {June},
  pages = {1--73},
  title = {Implicit {{Self-Regularization}} in {{Deep Neural Networks}}: {{Evidence}} from {{Random Matrix Theory}} and {{Implications}} for {{Learning}}},
  year = {2021}
}

@inproceedings{moura_lean_4_2021,
  address = {Berlin, Heidelberg},
  author = {de Moura, Leonardo and Ullrich, Sebastian},
  booktitle = {Automated {{Deduction}} -- {{CADE}} 28},
  doi = {10.1007/978-3-030-79876-5_37},
  isbn = {978-3-030-79875-8},
  month = {July},
  pages = {625--635},
  publisher = {Springer-Verlag},
  title = {The {{Lean}} 4 {{Theorem Prover}} and {{Programming Language}}},
  year = {2021}
}

@inproceedings{neyshabur_exploring_generalization_2017,
  author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and Mcallester, David and Srebro, Nati},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  title = {Exploring {{Generalization}} in {{Deep Learning}}},
  volume = {30},
  year = {2017}
}

@inproceedings{nguyen_leep_new_2020,
  author = {Nguyen, Cuong and Hassner, Tal and Seeger, Matthias and Archambeau, Cedric},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  issn = {2640-3498},
  pages = {7294--7305},
  publisher = {PMLR},
  title = {{{LEEP}}: {{A New Measure}} to {{Evaluate Transferability}} of {{Learned Representations}}},
  year = {2020}
}

@misc{ni_survey_large_2025,
  author = {Ni, Shiwen and Chen, Guhong and Li, Shuaimin and Chen, Xuanang and Li, Siyi and Wang, Bingli and Wang, Qiyao and Wang, Xingjian and Zhang, Yifan and Fan, Liyang and others},
  doi = {10.48550/arXiv.2508.15361},
  month = {August},
  number = {2508.15361},
  publisher = {arXiv},
  title = {A {{Survey}} on {{Large Language Model Benchmarks}}},
  year = {2025}
}

@misc{pacchiardi_100_instances_2024,
  author = {Pacchiardi, Lorenzo and Cheke, Lucy G. and {Hern{\'a}ndez-Orallo}, Jos{\'e}},
  doi = {10.48550/arXiv.2409.03563},
  month = {September},
  number = {2409.03563},
  publisher = {arXiv},
  title = {100 Instances Is All You Need: Predicting the Success of a New {{LLM}} on Unseen Data by Testing on a Few Instances},
  year = {2024}
}

@misc{polo_sloth_scaling_2025,
  author = {Polo, Felipe Maia and Somerstep, Seamus and Choshen, Leshem and Sun, Yuekai and Yurochkin, Mikhail},
  doi = {10.48550/arXiv.2412.06540},
  month = {February},
  number = {2412.06540},
  publisher = {arXiv},
  title = {Sloth: Scaling Laws for {{LLM}} Skills to Predict Multi-Benchmark Performance across Families},
  year = {2025}
}

@misc{polo_tinybenchmarks_evaluating_2024,
  author = {Polo, Felipe Maia and Weber, Lucas and Choshen, Leshem and Sun, Yuekai and Xu, Gongjun and Yurochkin, Mikhail},
  doi = {10.48550/arXiv.2402.14992},
  month = {May},
  number = {2402.14992},
  publisher = {arXiv},
  title = {{{tinyBenchmarks}}: Evaluating {{LLMs}} with Fewer Examples},
  year = {2024}
}

@inproceedings{ruan_observational_scaling_2024,
  author = {Ruan, Yangjun and Maddison, Chris J. and Hashimoto, Tatsunori},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  pages = {15841--15892},
  title = {Observational {{Scaling Laws}} and the {{Predictability}} of {{Language Model Performance}}},
  volume = {37},
  year = {2024}
}

@misc{shafto_aiq_artificial_2025,
  author = {Shafto, Patrick},
  howpublished = {https://www.darpa.mil/research/programs/aiq-artificial-intelligence-quantified},
  title = {{{AIQ}}: {{Artificial Intelligence Quantified}} {\textbar} {{DARPA}}},
  year = {2025}
}

@misc{shi_importance_sampling_2025,
  author = {Shi, Junjie and Ma, Wei and Ying, Shi and Jiang, Lingxiao and {liu}, Yang and Du, Bo},
  doi = {10.48550/arXiv.2508.01203},
  month = {August},
  number = {2508.01203},
  publisher = {arXiv},
  title = {Importance {{Sampling}} Is {{All You Need}}: {{Predict LLM}}'s Performance on New Benchmark by Reusing Existing Benchmark},
  year = {2025}
}

@inproceedings{shirishkeskar_largebatch_training_2017,
  author = {Shirish Keskar, Nitish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  doi = {10.48550/arXiv.1609.04836},
  title = {On {{Large-Batch Training}} for {{Deep Learning}}: {{Generalization Gap}} and {{Sharp Minima}}},
  year = {2017}
}

@article{srivastava_imitation_game_2023,
  author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and {Garriga-Alonso}, Adri{\`a} and others},
  issn = {2835-8856},
  journal = {Transactions on Machine Learning Research},
  month = {January},
  title = {Beyond the {{Imitation Game}}: {{Quantifying}} and Extrapolating the Capabilities of Language Models},
  year = {2023}
}

@inproceedings{wang_glue_multitask_2019,
  author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  title = {{{GLUE}}: {{A Multi-Task Benchmark}} and {{Analysis Platform}} for {{Natural Language Understanding}}},
  year = {2019}
}

@inproceedings{wang_superglue_stickier_2019,
  author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  title = {{{SuperGLUE}}: {{A Stickier Benchmark}} for {{General-Purpose Language Understanding Systems}}},
  volume = {32},
  year = {2019}
}

@inproceedings{yang_wide_feedforward_2019,
  author = {Yang, Greg},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  title = {Wide {{Feedforward}} or {{Recurrent Neural Networks}} of {{Any Architecture}} Are {{Gaussian Processes}}},
  volume = {32},
  year = {2019}
}

@inproceedings{you_logme_practical_2021,
  author = {You, Kaichao and Liu, Yong and Wang, Jianmin and Long, Mingsheng},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  issn = {2640-3498},
  pages = {12133--12143},
  publisher = {PMLR},
  title = {{{LogME}}: {{Practical Assessment}} of {{Pre-trained Models}} for {{Transfer Learning}}},
  year = {2021}
}

@techreport{finney_openpgp_message_2007,
  type = {Request for {{Comments}}},
  title = {{{OpenPGP Message Format}}},
  author = {Finney, Hal and Donnerhacke, Lutz and Callas, Jon and Thayer, Rodney L. and Shaw, Daphne},
  year = {2007},
  month = nov,
  number = {RFC 4880},
  institution = {Internet Engineering Task Force},
  doi = {10.17487/RFC4880},
  urldate = {2025-09-04},
  abstract = {This document is maintained in order to publish all necessary information needed to develop interoperable applications based on the OpenPGP format. It is not a step-by-step cookbook for writing an application. It describes only the format and methods needed to read, check, generate, and write conforming packets crossing any network. It does not deal with storage and implementation questions. It does, however, discuss implementation issues necessary to avoid security flaws. OpenPGP software uses a combination of strong public-key and symmetric cryptography to provide security services for electronic communications and data storage. These services include confidentiality, key management, authentication, and digital signatures. This document specifies the message formats used in OpenPGP. [STANDARDS-TRACK]},
  file = {/home/joncrall/Zotero/storage/QVPHIXXT/Finney et al. - 2007 - OpenPGP Message Format.pdf}
}


@inproceedings{chu_unified_framework_2023,
  title = {A Unified Framework for Information-Theoretic Generalization Bounds},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chu, Yifeng and Raginsky, Maxim},
  year = {2023},
  volume = {36},
  pages = {79260--79278},
  urldate = {2025-08-28},
  langid = {english},
}

@inproceedings{schoenholz_deep_information_2017,
  title = {Deep {{Information Propagation}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Schoenholz, Samuel S. and Gilmer, Justin and Ganguli, Surya and {Sohl-Dickstein}, Jascha},
  year = {2017},
  eprint = {1611.01232},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1611.01232},
  urldate = {2025-09-04},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/joncrall/Zotero/storage/S39GZQ9N/Schoenholz et al. - 2017 - Deep Information Propagation.pdf;/home/joncrall/Zotero/storage/NPWEXMMS/1611.html}
}
